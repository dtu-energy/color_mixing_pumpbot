{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on Bayesian Optimization\n",
    "\n",
    "[Bayesian optimization](https://en.wikipedia.org/wiki/Bayesian_optimization) is an optimization method that uses Bayes Theorem to _direct_ the search to find the minimum/maximum of an objective function. \n",
    "\n",
    "Let's start by recalling that Bayes Theorem is an approach for calculating the conditional probability of an event:\n",
    "\\begin{equation}\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)}.\n",
    "\\end{equation}\n",
    "The expression can be simplified by removing the normalization factor $P(B)$, which still allows us to achieve our goal of optimizing a quantity by describing the conditional probability as a proportional quantity. \n",
    "\\begin{equation}\n",
    "P(A|B) = P(B|A) P(A).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can devise specific samples $\\{x_1, x_2, \\cdots, x_n\\}$ and evaluate them using the objective function $f(x_i)$ that returns the cost or outcome for the sample $x_i$. Samples and their outcome are collected sequentially and define our data $D$, e.g. $D = \\{x_i, f(x_i), \\cdots, x_n, f(x_n)\\}$ and is used to define the prior. The likelihood function is defined as the probability of observing the data given the function $P(D|f)$. This likelihood function will change as more observations are collected.\n",
    "\n",
    "\\begin{equation}\n",
    "P(f|D) = F(D|f) P(f)\n",
    "\\end{equation}\n",
    "The posterior represents everything we know about the objective function, and it is an approximation of the objective function and can be used to estimate the value of $f()$ of different candidate samples. In this way, the posterior probability is a surrogate objective function. \n",
    "\n",
    "**Surrogate Function**: Bayesian approximation of the objective function that can be sampled efficiently.\n",
    "\n",
    "The surrogate function gives us an estimate of the objective function, which can be used to direct future sampling. Sampling involves careful use of the posterior in a function known as the \"acquisition\" function, e.g. for acquiring more samples. We want to use our belief about the objective function to sample the area of the search space that is most likely to pay off (either as an exploration or exploitation). \n",
    "\n",
    "**Acquisition Function**: Technique by which the posterior is used to select the next sample from the search space. \n",
    "\n",
    "The acquisition function will optimize the conditional probability of locations in the search to generate the next sample. Once additional samples and their evaluation via the objective function $f()$ have been collected, they are added to data $D$ and the posterior is then updated. \n",
    "\n",
    "This process is repeated until the extrema of the objective function are located, a good enough results is located, or resources are exhausted. \n",
    "\n",
    "The Bayesian Optimization algorithm can be summarized as follows:\n",
    "\n",
    "1. Select a sample by optimizing the acquisition function\n",
    "2. Evaluate the sampel with the objective function \n",
    "3. Update the data and, in turn, the surrogate function\n",
    "4. Go to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to perform Bayesian Optimization\n",
    "\n",
    "We will now explore how Bayesian Optimization works by developing an implementation from scratch for a simple one-dimensional \n",
    "test function. \n",
    "\n",
    "First, we will define the test problem, then how to model the mapping of inputs to outputs with a surrogate function. \n",
    "Next, we will see how the surrogate function can be searched efficiently with an acuqisition function before tying \n",
    "all of these elements together into the Bayesian Optimization procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Problem\n",
    "\n",
    "We will use a multimodal problem with five peaks, calculated as\n",
    "\\begin{equation}\n",
    "y = x^2 \\sin^6(5 \\pi x),\n",
    "\\end{equation}\n",
    "where $x$ is a real value in the range [0, 1).\n",
    "\n",
    "We will augment this function by adding Gaussian noise with a mean of zero and a standard deviation of 0.1. This will mean that the real evaluation will have a positive or negative random value added to it, making the function challenging to optimize.\n",
    "\n",
    "The `objective()` function below implements this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import normal\n",
    "from math import sin\n",
    "from math import pi\n",
    "\n",
    "# objective function\n",
    "def objective(x: float) -> float:\n",
    "    return (x**2 * sin(5 * pi * x)**6.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this function by first defining a grid-based sample of inputs from 0 to 1 with a step size of 0.01 across the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import arange\n",
    "# grid-based sample of the domain [0,1)\n",
    "X = arange(0, 1, 0.01)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then evaluate these samples using the target function to see what the objective function looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the domain \n",
    "y = [objective(x) for x in X]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would not know this in practice, but for out test problem, it is good to know the real best input and output of the function to see if the Bayesian Optimization algorithm can locate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "# find best result\n",
    "ix = argmax(y)\n",
    "print(f\"Optima: x={X[ix]:.3}, y={y[ix]:.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global optima is an input with the value 0.9 that gives the score 0.81.\n",
    "\n",
    "Finally, we can create a plot, first showing the evaluation as a scatter plot with input on the x-axis and score on the y-axis, then a line plot of the scores without any noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "# plot the points\n",
    "pyplot.plot(X, y)\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surrogate Function \n",
    "\n",
    "The surrogate function is a technique used to best approximate the mapping of design parameters to an output score. It summarizes the conditional probability of an objective function ($f$), given the available data ($D$) or $P(f|D)$.\n",
    "\n",
    "We use Gaussian Process (GP) construct a joint probability distribution over the variables, assuming a multivariate Gaussian distribution. As such, it is capable of efficient and effective summarization of a large number of functions and smooth transition as more observations are made available to the model.\n",
    "\n",
    "This smooth structure and smooth transition to new functions based on data are desirable properties as we sample the domain, and the multivariate Gaussian basis to the model means that an estimate from the model will be a mean of a distribution with a standard deviation; that will be helpful later in the acquisition function.\n",
    "\n",
    "We can fit a GP regression model using the [GaussianProcessRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html) scikit-learn implementation from a sample of inputs ($X$) and evaluations from the objective function ($y$).\n",
    "\n",
    "First, the model must be defined. An important aspect in defining the GP model is the kernel. This controls the shape of the function at specific points based on distance measures between actual data observations. Many different kernel functions can be used, and some may offer better performance for specific datasets.\n",
    "\n",
    "By default, a [Radial Basis Function](https://en.wikipedia.org/wiki/Radial_basis_function), or RBF, is used that can work well.\n",
    "\n",
    "```python\n",
    "# define the model\n",
    "model = GaussianProcessRegressor()\n",
    "```\n",
    "\n",
    "Once defined, the model can be fit on the training dataset directly by calling the `fit()` function. \n",
    "\n",
    "```python\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "The model will estimate the cost for one or more samples provided to it.\n",
    "\n",
    "The model is used by calling the `predict()` function. The result for a given sample will be a mean of the distribution at that point. We can also get the standard deviation of the distribution at that point in the function by specifying the argument `return_std=True`. For example:\n",
    "\n",
    "```python\n",
    "yhat = model.predict(X, return_std=True)\n",
    "```\n",
    "\n",
    "This function can result in warnings if the distribution is thin at a given point we are interested in sampling. Therefore, we silence all of the warnings when making a prediction. The `surrogate()` function below takes the fit model and one or more samples and returns the mean and standard deviation estimated costs without printing any warnings.\n",
    "\n",
    "```python\n",
    "# surrogate or approximation for the objective function\n",
    "def surrogate(model, X):\n",
    "    # catch any warning generated when making a prediction\n",
    "    with catch_warnings():\n",
    "        # ignore generated warnings\n",
    "        simplefilter(\"ignore\")\n",
    "        return model.predict(X, return_std=True)\n",
    "```\n",
    "\n",
    "We can call this function any time to estimate the cost of one or more samples, such as when we want to optimize the acquisition function in the next section. For now, it is interesting to see what the surrogate function looks like across the domain after it is trained on a random sample. \n",
    "\n",
    "We achieve this by first fitting the GP model on a random sample of 100 data points and their real objective function values. We can then plot a scatter plot of these points. Next, we can perform a grid-based sample across the input domain and estimate the cost at each point using the surrogate function and plot the result as a line.\n",
    "\n",
    "We would expect the surrogate function to have a crude approximation of the true objective function. The `plot()` function below creates this plot, given the random data sample of the objective function and the fit model.\n",
    "\n",
    "```python\n",
    "# plot real observations vs surrogate function\n",
    "def plot(X, y, model):\n",
    "    # scatter plot of inputs and real objective function\n",
    "    pyplot.scatter(X, y)\n",
    "    # line plot of surrogate function across domain\n",
    "    Xsamples = asarray(arange(0, 1, 0.001))\n",
    "    Xsamples = Xsamples.reshape(len(Xsamples), 1)\n",
    "    ysamples, _ = surrogate(model, Xsamples)\n",
    "    pyplot.plot(Xsamples, ysamples)\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "```\n",
    "\n",
    "Tying this together, the complete example of fitting a Gaussian Process regression model and plotting the sample vs. the surrogate function is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a gaussian process surrogate function\n",
    "from math import sin\n",
    "from math import pi\n",
    "from numpy import arange\n",
    "from numpy import asarray\n",
    "from numpy.random import normal\n",
    "from numpy.random import random\n",
    "from matplotlib import pyplot\n",
    "from warnings import catch_warnings\n",
    "from warnings import simplefilter\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    " \n",
    "# objective function\n",
    "def objective(x):\n",
    "    return (x**2 * sin(5 * pi * x)**6.0)\n",
    " \n",
    "# surrogate or approximation for the objective function\n",
    "def surrogate(model, X):\n",
    "    # catch any warning generated when making a prediction\n",
    "    with catch_warnings():\n",
    "        # ignore generated warnings\n",
    "        simplefilter(\"ignore\")\n",
    "        return model.predict(X, return_std=True)\n",
    " \n",
    "# plot real observations vs surrogate function\n",
    "def plot(X, y, model):\n",
    "    # scatter plot of inputs and real objective function\n",
    "    pyplot.scatter(X, y)\n",
    "    # line plot of surrogate function across domain\n",
    "    Xsamples = asarray(arange(0, 1, 0.001))\n",
    "    Xsamples = Xsamples.reshape(len(Xsamples), 1)\n",
    "    ysamples, _ = surrogate(model, Xsamples)\n",
    "    pyplot.plot(Xsamples, ysamples)\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    " \n",
    "# sample the domain sparsely with noise\n",
    "X = random(100)\n",
    "y = asarray([objective(x) for x in X])\n",
    "# reshape into rows and cols\n",
    "X = X.reshape(len(X), 1)\n",
    "y = y.reshape(len(y), 1)\n",
    "# define the model\n",
    "model = GaussianProcessRegressor()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# plot the surrogate function\n",
    "plot(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first draws the random sample, evaluates it with the objective function, then fits the GP model.\n",
    "\n",
    "The data samples are potted as dots and the surrogate function is plotted as a line.\n",
    "\n",
    "As we expected, the plot resembles a crude version of the underlying objective function, importantly with a peak around 0.9 where we know the true maxima is located.\n",
    "\n",
    "Next, we must define a strategy for sampling the surrogate function.\n",
    "\n",
    "## Acquisition Function\n",
    "\n",
    "The surrogate function is used to test a range of candidate samples in the domain. Using the surrogate model results, candidate(s) can be selected and evaluated with the real, computationally expensive cost function.\n",
    "\n",
    "This involves two pieces: the search strategy used to navigate the domain in response to the surrogate function and the acquisition function that is used to interpret and score the response from the surrogate function.\n",
    "\n",
    "In this example, we will use a random search in order to keep the example simple. This involves first drawing a random sample of candidate samples from the domain, evaluating them with the acquisition function, then maximizing the acquisition function or choosing the candidate sample that gives the best score. The `opt_acquisition()` function below implements this.\n",
    "\n",
    "```python\n",
    "# optimize the acquisition function\n",
    "def opt_acquisition(X, y, model):\n",
    "    # random search, generate random samples\n",
    "    Xsamples = random(100)\n",
    "    Xsamples = Xsamples.reshape(len(Xsamples), 1)\n",
    "    # calculate the acquisition function for each sample\n",
    "    scores = acquisition(X, Xsamples, model)\n",
    "    # locate the index of the largest scores\n",
    "    ix = argmax(scores)\n",
    "    return Xsamples[ix, 0]\n",
    "```\n",
    "\n",
    "The acquisition function is responsible for scoring or estimating the likelihood that a given candidate sample (input) is worth evaluating with the real objective function.\n",
    "\n",
    "We could just use the surrogate score directly. Alternately, given that we have chosen a Gaussian Process model as the surrogate function, we can use the probabilistic information from this model in the acquisition function to calculate the probability that a given sample is worth evaluating.\n",
    "\n",
    "There are many different types of probabilistic acquisition functions that can be used, each providing a different trade-off for how exploitative (greedy) and explorative they are.\n",
    "\n",
    "Three common examples include:\n",
    "\n",
    "* Probability of Improvement (PI).\n",
    "* Expected Improvement (EI).\n",
    "* Lower Confidence Bound (LCB).\n",
    "\n",
    "The Probability of Improvement (PI) method is the simplest, whereas the Expected Improvement method is the most commonly used. In this exercise, we will use the simpler PI method, which is calculated as the normal cumulative probability of the normalized expected improvement. It is calculated as follows:\n",
    "\n",
    "$\\mathrm{PI} = \\mathrm{cdf}((\\mu â€“ \\mu_{best})/\\mathrm{stdev})$\n",
    "\n",
    "\n",
    "$\\mathrm{cdf}()$ is the normal cumulative distribution function, $\\mu$ is the mean of the surrogate function for a given sample $x$, $\\mathrm{stdev}$ is the standard deviation of the surrogate function for a given sample $x$, and $\\mu_{best}$ is the mean of the surrogate function for the best sample found so far.\n",
    "\n",
    "We can add a very small number to the standard deviation to avoid a divide by zero error.\n",
    "\n",
    "The `acquisition()` function below implements this given the current training dataset of input samples, an array of new candidate samples, and the fit GP model.\n",
    "\n",
    "```python\n",
    "# probability of improvement acquisition function\n",
    "def acquisition(X, Xsamples, model):\n",
    "    # calculate the best surrogate score found so far\n",
    "    yhat, _ = surrogate(model, X)\n",
    "    best = max(yhat)\n",
    "    # calculate mean and stdev via surrogate function\n",
    "    mu, std = surrogate(model, Xsamples)\n",
    "    mu = mu[:, 0]\n",
    "    # calculate the probability of improvement\n",
    "    probs = norm.cdf((mu - best) / (std+1E-9))\n",
    "    return probs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Bayesian Optimization Algorithm\n",
    "\n",
    "We can tie all of this together into the Bayesian Optimization algorithm.\n",
    "\n",
    "The main algorithm involves cycles of selecting candidate samples, evaluating them with the objective function, then updating the GP model.\n",
    "\n",
    "```python\n",
    "# perform the optimization process\n",
    "for i in range(100):\n",
    "    # select the next point to sample\n",
    "    x = opt_acquisition(X, y, model)\n",
    "    # sample the point\n",
    "    actual = objective(x)\n",
    "    # summarize the finding for our own reporting\n",
    "    est, _ = surrogate(model, [[x]])\n",
    "    print('>x=%.3f, f()=%3f, actual=%.3f' % (x, est, actual))\n",
    "    # add the data to the dataset\n",
    "    X = vstack((X, [[x]]))\n",
    "    y = vstack((y, [[actual]]))\n",
    "    # update the model\n",
    "    model.fit(X, y)\n",
    "```\n",
    "The complete code is shown below.  Running the example first creates an initial random sample of the search space and evaluation of the results. Then a GP model is fit on this data.\n",
    "\n",
    "Your specific results will vary given the stochastic nature of the sampling of the domain. Try running the example a few times.\n",
    "\n",
    "A plot is created showing the raw observations as dots and the surrogate function across the entire domain. In this case, the initial sample has a good spread across the domain and the surrogate function has a bias towards the part of the domain where we know the optima is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin\n",
    "from math import pi\n",
    "from numpy import arange\n",
    "from numpy import vstack\n",
    "from numpy import argmax\n",
    "from numpy import asarray\n",
    "from numpy.random import normal\n",
    "from numpy.random import random\n",
    "from scipy.stats import norm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from warnings import catch_warnings\n",
    "from warnings import simplefilter\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# objective function\n",
    "def objective(x):\n",
    "\treturn (x**2 * sin(5 * pi * x)**6.0)\n",
    "\n",
    "# surrogate or approximation for the objective function\n",
    "def surrogate(model, X):\n",
    "    # catch any warning generated when making a prediction\n",
    "    with catch_warnings():\n",
    "        # ignore generated warnings\n",
    "        simplefilter(\"ignore\")\n",
    "        return model.predict(X, return_std=True)\n",
    "\n",
    "# probability of improvement acquisition function\n",
    "def acquisition(X, Xsamples, model):\n",
    "    # calculate the best surrogate score found so far\n",
    "    yhat, _ = surrogate(model, X)\n",
    "    best = max(yhat)\n",
    "    # calculate mean and stdev via surrogate function\n",
    "    mu, std = surrogate(model, Xsamples)\n",
    "    # calculate the probability of improvement\n",
    "    probs = norm.cdf((mu - best) / (std+1E-9))\n",
    "    return probs\n",
    "\n",
    "# optimize the acquisition function\n",
    "def opt_acquisition(X, y, model):\n",
    "    # random search, generate random samples\n",
    "    Xsamples = random(100)\n",
    "    Xsamples = Xsamples.reshape(len(Xsamples), 1)\n",
    "    # calculate the acquisition function for each sample\n",
    "    scores = acquisition(X, Xsamples, model)\n",
    "    # locate the index of the largest scores\n",
    "    ix = argmax(scores)\n",
    "    return Xsamples[ix, 0]\n",
    "\n",
    "# plot real observations vs surrogate function\n",
    "def plot(X, y, model):\n",
    "    # scatter plot of inputs and real objective function\n",
    "    pyplot.scatter(X, y)\n",
    "    # line plot of surrogate function across domain\n",
    "    Xsamples = asarray(arange(0, 1, 0.001))\n",
    "    Xsamples = Xsamples.reshape(len(Xsamples), 1)\n",
    "    ysamples, _ = surrogate(model, Xsamples)\n",
    "    pyplot.plot(Xsamples, ysamples)\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "\n",
    "# sample the domain sparsely with noise\n",
    "X = random(100)\n",
    "y = asarray([objective(x) for x in X])\n",
    "# reshape into rows and cols\n",
    "X = X.reshape(len(X), 1)\n",
    "y = y.reshape(len(y), 1)\n",
    "# define the model\n",
    "model = GaussianProcessRegressor()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# plot before hand\n",
    "plot(X, y, model)\n",
    "# perform the optimization process\n",
    "for i in range(100):\n",
    "    # select the next point to sample\n",
    "    x = opt_acquisition(X, y, model)\n",
    "    # sample the point\n",
    "    actual = objective(x)\n",
    "    # summarize the finding\n",
    "    est, _ = surrogate(model, [[x]])\n",
    "    print(f\">x={x:.3}, f()={est[0]:.3}, actual={actual:.3}\")\n",
    "    # add the data to the dataset\n",
    "    X = vstack((X, [[x]]))\n",
    "    y = vstack((y, [[actual]]))\n",
    "    # update the model\n",
    "    model.fit(X, y)\n",
    "\n",
    "# plot all samples and the final surrogate function\n",
    "plot(X, y, model)\n",
    "# best result\n",
    "ix = argmax(y)\n",
    "print(f\"Best Result: x={X[ix][0]:.3}, y={y[ix][0]:.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the best input and its objective function score are reported. We know the optima has an input of 0.9 and an output of 0.810."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1bceae1b0f30ee21e0b335445df0019c1035ce0f130113777dc9bc85664ba25"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('47332-2022': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
