{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Correcting Neural Network\n",
    "\n",
    "### TL:DR\n",
    "\n",
    "1. Mix each color using `pumpbot` and measure color values\n",
    "    - Also mix [0.25, 0.25, 0.25, 0.25] on `pumpbot`\n",
    "2. Set `true_coefficients`of `silicobot` to the individual `pumpbot` colors.\n",
    "    - Also Mix [0.25, 0.25, 0.25, 0.25] on `silicobot` and measure difference to `pumpbot`\n",
    "3. Do Bayesian Optimization in silico by changing weights $w$\n",
    "    - Input: $w \\cdot [0.25, 0.25, 0.25, 0.25]$ in silico\n",
    "    - Output: $\\mathrm{diff}(\\text{weighted silicobot measured color}, \\text{pumpbot measured color})$\n",
    "4. Use best weight set and silicobot to create an ML that models the difference between `pumpbot` and `silicobot` using previous data from `pumpbot`:\n",
    "    - Input: $w^* \\cdot color$ in silico\n",
    "    - Output: `pumpbot` mixed color - `silicobot` mixed color\n",
    "    - This model can now take a silico mixed color and output the difference between this and the pumpbot color. This method requires that you have a bunch of `pumpbot` data\n",
    "    - You can use Bayesian optimization to optimize number of hidden layers and hidden layer sizes of the NN model\n",
    "5a. Generate data using error corrected `silicobot`:\n",
    "    - 1000 random mixtures multiplied by best weights found in **5**.\n",
    "    - Mixed colors are \"measured\" on the `silicobot`. Run the mixtures through the Error correcting ML model and add the resulting errors to the \"measured\" colors.\n",
    "5b. Reverse the data and train a new model:\n",
    "    - The measured colors (including error correction) are now the input to an new ML model.\n",
    "    - The output is the mixture values.\n",
    "6. Apply the reverse model:\n",
    "    1. Measure random target color.\n",
    "    2. Input measured RGB value in NN model\n",
    "    3. Use NN model output mixture to mix color.\n",
    "    4. Measure the newly mixed color and compare to target color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing the required packages for PumpController, SilicoPumpController, Odyssey and more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PumpController and SilicoPumpController\n",
    "from pump_controller import PumpController, get_serial_port, list_serial_ports\n",
    "from pump_controller import SilicoPumpController\n",
    "from pump_controller import visualize_rgb, visualize_candidates\n",
    "from pump_controller import read_logfile\n",
    "\n",
    "# Odyssey\n",
    "from odyssey.mission import Mission # Mission\n",
    "from odyssey.navigators import SingleGP_Navigator # Navigator\n",
    "from odyssey.navigators.sampler_navigators import Sobol_Navigator # Sampler\n",
    "from odyssey.navigators import ExpectedImprovement # Acquisition\n",
    "from odyssey.objective import Objective # Objective\n",
    "\n",
    "# Other Packages\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt  \n",
    "from IPython import display\n",
    "from warnings import catch_warnings, simplefilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the `color_difference` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between mixed and target colors:\n",
    "def color_difference(mixed_color, target_color):\n",
    "\n",
    "    mixed_color = np.array(mixed_color)\n",
    "    target_color = np.array(target_color)\n",
    "    # Calculate the sum of root mean squared differences between mixed color and target color\n",
    "    rmse = np.sqrt(np.mean((mixed_color - target_color)**2, axis=-1))\n",
    "    return np.sum(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mix each individual color using pumpbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mix pure red, green, blue and yellow on the pump bot and plot and store these colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumpbot = PumpController(ser_port = get_serial_port(), cell_volume = 20.0, drain_time = 20.0, config_file = 'config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumpbot_mixed_colors = []\n",
    "for i in range(4):\n",
    "    color_to_mix = [0,0,0,0]\n",
    "    color_to_mix[i] = 1\n",
    "\n",
    "    mixed_color = pumpbot.mix_color(color_to_mix)\n",
    "    \n",
    "    pumpbot_mixed_colors.append(mixed_color)\n",
    "\n",
    "# Plot the colors\n",
    "fig, axs = plt.subplots(1, 4, figsize=(12, 3))  # Create a 1x4 grid of subplots\n",
    "\n",
    "for i, color in enumerate(pumpbot_mixed_colors):\n",
    "    # Reshape the color to a 1x1x3 array and normalize to [0, 1]\n",
    "    color = color.reshape(1, 1, 3) / 255.0\n",
    "\n",
    "    axs[i].imshow(color)  # Display the color\n",
    "    axs[i].axis('off')  # Hide the axes\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, mix equal amounts of each color on pumpbot and visualize this color. We will use the `equal_color_mixture` variable multiple times during the course of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_color_mixture = [0.25,0.25,0.25,0.25]\n",
    "pumpbot_equal_color = pumpbot.mix_color(equal_color_mixture)\n",
    "\n",
    "visualize_rgb(mixture = equal_color_mixture, \n",
    "                  rgb = pumpbot_equal_color,\n",
    "                  pump_controller = pumpbot,\n",
    "                  target = None)\n",
    "\n",
    "print(pumpbot_equal_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a silicobot using the colors obtained from pumpbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the silicobot, but with the newly obtained `pumpbot_mixed_colors` as the `true_coefficients` of the silicobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silicobot = SilicoPumpController(noise_std = 0, true_coefficients = pumpbot_mixed_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mix equal amounts of each color on the silicobot. You can use the `equal_color_mixture` variable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silicobot_equal_color = silicobot.mix_color(equal_color_mixture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the mixed color and compare it to the color obtained on the pumpbot. This is an indicator of the difference between the pumpbot and the silicobot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = color_difference(silicobot_equal_color, pumpbot_equal_color)\n",
    "\n",
    "visualize_rgb(mixture = equal_color_mixture, \n",
    "                  rgb = silicobot_equal_color,\n",
    "                  pump_controller = silicobot,\n",
    "                  target = pumpbot_equal_color,\n",
    "                  score = score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Weights Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now optimize the weights of the silicobot. The objective will be to optimize the difference between the silicobot and pumpbot, when mixing the same color (equal amounts of each color). The parameters we will be tuning are the weights $w = [w_1, w_2, w_3, w_4]^T$, so that the color mixture mixed by the silicobot is $w \\cdot [0.25, 0.25, 0.25, 0.25]$.\n",
    "\n",
    "We can do this using Bayesian Optimization through Odyssey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_weights(weights):\n",
    "    weighted_silicobot_equal_color = silicobot.mix_color(weights * torch.tensor([0.25, 0.25, 0.25, 0.25]))\n",
    "    score = color_difference(weighted_silicobot_equal_color, pumpbot_equal_color)\n",
    "    return score\n",
    "\n",
    "objective = Objective(find_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goals = ['descend']\n",
    "\n",
    "min = 1e-5\n",
    "\n",
    "param_space = ([min, 1.0], [min, 1.0], [min, 1.0], [min, 1.0])\n",
    "\n",
    "# Define Mission\n",
    "mission = Mission(name = 'Silico Error Correcting',\n",
    "                  funcs = [objective],\n",
    "                  maneuvers = goals,\n",
    "                  envelope = param_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_init_design = 5\n",
    "navigator = SingleGP_Navigator(mission = mission,\n",
    "                               num_init_design = num_init_design,\n",
    "                               init_method = Sobol_Navigator(mission = mission),\n",
    "                               input_scaling = False,\n",
    "                               data_standardization = False,\n",
    "                               display_always_max = False,\n",
    "                               acq_function_type = ExpectedImprovement,\n",
    "                               acq_function_params = {'best_f': 0.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 30\n",
    "while len(mission.train_X) - num_init_design < num_iter:\n",
    "\n",
    "    with catch_warnings() as w:\n",
    "        simplefilter('ignore')\n",
    "        \n",
    "        trajectory = navigator.trajectory()\n",
    "        observation = navigator.probe(trajectory, init = False)\n",
    "\n",
    "        navigator.relay(trajectory, observation)\n",
    "        navigator.upgrade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the different iterations and the score, and find the weights that allowed for the lowest difference (score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = mission.display_X\n",
    "scores = mission.display_Y\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.scatter(range(len(scores)), scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best weights (the ones where the score is lowest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = scores.argmin().item()\n",
    "best_weights = inputs[best_idx]\n",
    "best_score = scores[best_idx]\n",
    "\n",
    "print(best_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mix equal amounts of this color, but this time weighted by the best weights found using the previous bayesian optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weighted_mixed_color = silicobot.mix_color(best_weights * torch.tensor([0.25, 0.25, 0.25, 0.25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize this mixed color and compare to the equal mixtures color achieved on the pumpbot. Hopefully the difference is lower after using the optimized weights! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = color_difference(best_weighted_mixed_color, pumpbot_equal_color)\n",
    "\n",
    "print(f'Weights: {torch.round(best_weights, decimals = 3)}')\n",
    "visualize_rgb(mixture = equal_color_mixture, \n",
    "                  rgb = best_weighted_mixed_color,\n",
    "                  pump_controller = silicobot,\n",
    "                  target = pumpbot_equal_color,\n",
    "                  score = score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Accumulate Old Data or Generate New Data for Error Correction NN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we are creating a neural network model, that can predict the error between our silicobot and the pumpcontroller to get the error even further down. We do this by combining all the data that we have from previous runs of the pumpbot, and using those mixtures as mixtures for the silicobot (weighted by the weights found earlier), and then calculating the error between the color that the pumpbot previously achieved and what the silicobot outputs. Using this data, we can train a neural network model. The more data that we have from the pumpbot, the more accurate this model will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a. Load Old Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all your files in one folder and load them  all automatically, or manually make a list of your previous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "files = glob('student_data/*.csv') # Get all files in a directory\n",
    "# files = ['datafile1.csv', 'datafile2.csv', 'datafile3.csv'] # Or specify the files manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the dataframes, remove nan rows and restructure the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "for i in range(len(files)):\n",
    "    if i == 0:\n",
    "        df = pd.read_csv(files[i])\n",
    "    else:\n",
    "        df = pd.concat([df, pd.read_csv(files[i])])\n",
    "\n",
    "df = df.dropna() # Remove any nan rows\n",
    "df = df.reset_index(drop=True) # Reset index\n",
    "\n",
    "for column in df.columns:\n",
    "        df[column] = df[column].apply(ast.literal_eval)\n",
    "\n",
    "# Convert the lists of strings into lists of floats\n",
    "for column in df.columns:\n",
    "    df[column] = df[column].apply(lambda x: np.array([float(i) for i in x]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data for this model is now generated by using the mixtures from the pumpbot, and weighted by the best weights found for the silicobot, and normalizing these values.\n",
    "\n",
    "The output data is the error between the silicobot and the pumpbot for these mixtures. This can be calculated by running the silicobot on the mixture and subtracting this color from the pumpbot color made by using the same mixture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If data is loaded \n",
    "\n",
    "X = torch.tensor(df.mixture) # Get the input data from pumpbot results\n",
    "X = best_weights * X # Scale the points with the best weights\n",
    "X /= X.sum(dim=1, keepdim=True) # Normalize the points\n",
    "\n",
    "\n",
    "Y = torch.tensor(df.measurement)\n",
    "for i in range(len(Y)):\n",
    "    error = Y[i] - torch.tensor(silicobot.mix_color(X[i]))\n",
    "    Y[i] = error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4b. ... or Create New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have previous data (or want to add more data), you can generate it here as well, but this could take a while as it involves running the pumpbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If data is generated \n",
    "\n",
    "X = torch.rand(60, 4) # Generate random points\n",
    "X = best_weights * X # Scale the points with the best weights\n",
    "X /= X.sum(dim=1, keepdim=True) # Normalize the points\n",
    "\n",
    "Y = []\n",
    "for i in range(len(X)):\n",
    "    error = pumpbot.mix_color(X[i]) - silicobot.mix_color(X[i])\n",
    "    Y.append(error)\n",
    "\n",
    "Y = torch.stack(Y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using functions from pytorch, we can create a train-test split (here an 80-20 split), and create dataloaders (not to be confused by the Odyssey DataLoader) for the train and test data. These dataloaders get the data ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "\n",
    "# Create train, dependent test data\n",
    "dataset = TensorDataset(X, Y)\n",
    "\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_size, shuffle=True) # Use all data\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_size, shuffle=False) # Use all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4c. Create the Error Correction ML Model and Train Hyperparameters using Bayesian Optimization with Odyssey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a general class for an Multi-Layer-Perceptron Regressor. This class takes an input size (in this case 4 nodes - one for each mixture color RGBY), an output size (in this case 3 nodes - one for each measurement color RGB) and a list of hidden layers. As you know, a neural network has a certain number of hidden layers each with a certain amount of nodes, and these numbers can be passed using the `hidden_layers` argument as a list. For example, creating 3 hidden layers with 2,3 and 4 nodes, respectively:\n",
    "\n",
    "$$\\texttt{hidden\\_layers} = [2, 3, 4]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers, dropout_prob=0.5):\n",
    "        super(MLPRegressor, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        last_size = input_size\n",
    "        for size in hidden_layers:\n",
    "            if size > 0:  # Only add the layer if it has more than 0 nodes\n",
    "                self.layers.append(nn.Linear(last_size, size))\n",
    "                self.layers.append(nn.BatchNorm1d(size))\n",
    "                self.layers.append(nn.ReLU())\n",
    "                self.layers.append(nn.Dropout(dropout_prob))\n",
    "                last_size = size\n",
    "        self.layers.append(nn.Linear(last_size, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of hidden layers and nodes in each of them can be a mystery sometimes, but luckily we have a tool to help us search these kinds of spaces - Bayesian Optimization!\n",
    "\n",
    "We create an objective function for bayesian optimization in odyssey. This function creates a MLPRegressor model with 4 input layers and 3 output layers and with a given number of hidden layers with given amounts of nodes in each hidden layer. We set the model in train mode, and train it for 100 epochs. After training, the model with the given hidden layers is evaluated on the test data using the `test_loader`. The loss function used for model training and testing is the root mean squared error loss (rMSE loss ). When testing, the average RMSE loss over all tested batches is returned as the output of this function.\n",
    "\n",
    "To simplify, this function takes a list of hidden layers and their nodes as an input, and outputs the results of the model after training and testing. Using this function, we can optimize the number of hidden layers and nodes!\n",
    "\n",
    "Note: Odyssey optimizes assuming that the input data is continuous. Obviously, the number of hidden layers and their nodes are all discrete values. The workaround for this is to simply convert the suggested number of continuous hidden layer configurations to integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(hidden_layers):\n",
    "    # Round the number of nodes in each hidden layer to the nearest integer\n",
    "    hidden_layers = hidden_layers.to(torch.int64).squeeze()\n",
    "\n",
    "    # Create an instance of the MLP regressor\n",
    "    model = MLPRegressor(4, 3, hidden_layers)\n",
    "\n",
    "    model.train()\n",
    "    # Train the model\n",
    "    loss_fn = nn.MSELoss() # Mean squared error loss\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    n_epochs = 100\n",
    "    for epoch in range(n_epochs):\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss = torch.sqrt(loss) # turn mse to rmse loss for each batch\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    # Evaluate the model\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            total_loss += torch.sqrt(loss_fn(outputs, targets)) # turn mse to rmse loss for each batch\n",
    "\n",
    "    return total_loss / len(test_loader) # average rmse over all batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = Objective(train_and_evaluate_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, a maximum of 5 hidden layers is created, with each hidden layers having 0 to 20 nodes. If a layer has 0 nodes, it is obviously skipped. Now we can use odyssey to optimize the number of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goals = ['descend']\n",
    "\n",
    "min_hidden_layer_nodes = 0.0\n",
    "max_hidden_layer_nodes = 20.0\n",
    "max_n_hidden_layers = 5\n",
    "\n",
    "param_space = [[min_hidden_layer_nodes, max_hidden_layer_nodes]] * max_n_hidden_layers\n",
    "\n",
    "# Define Mission\n",
    "mission = Mission(name = 'Mix2RGB MLP Tuning',\n",
    "                  funcs = [objective],\n",
    "                  maneuvers = goals,\n",
    "                  envelope = param_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_init_design = 5\n",
    "navigator = SingleGP_Navigator(mission = mission,\n",
    "                               num_init_design = num_init_design,\n",
    "                               init_method = Sobol_Navigator(mission = mission),\n",
    "                               input_scaling = False,\n",
    "                               data_standardization = False,\n",
    "                               display_always_max = False,\n",
    "                               acq_function_type = ExpectedImprovement,\n",
    "                               acq_function_params = {'best_f': 0.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 50\n",
    "while len(mission.train_X) - num_init_design < num_iter:\n",
    "\n",
    "    with catch_warnings() as w:\n",
    "        simplefilter('ignore')\n",
    "        \n",
    "        trajectory = navigator.trajectory()\n",
    "        observation = navigator.probe(trajectory, init = False)\n",
    "\n",
    "        navigator.relay(trajectory, observation)\n",
    "        navigator.upgrade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can figure out which configuration of hidden layer sizes resulted in the best results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = mission.display_Y.argmin()\n",
    "best_params = mission.display_X[best_idx]\n",
    "best_metric = mission.display_Y[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params.to(torch.int64))\n",
    "print(best_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the best achieved hidden layer configuration and retrain the model, this time for more epochs (more time consuming, but possibly more accurate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the best model\n",
    "forward_model = MLPRegressor(4, 3, best_params.to(torch.int64))\n",
    "\n",
    "forward_model.train()\n",
    "# Train the model\n",
    "loss_fn = nn.MSELoss() # Mean squared error loss\n",
    "\n",
    "optimizer = optim.Adam(forward_model.parameters())\n",
    "n_epochs = 500\n",
    "for epoch in range(n_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = forward_model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss = torch.sqrt(loss) # turn mse to rmse loss for each batch\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this model to predict the error correction between the silicobot and the pumpbot for the equal amounts case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = best_weights * torch.tensor([[0.25, 0.25, 0.25, 0.25]])\n",
    "\n",
    "forward_model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Temporarily turn off gradient computation\n",
    "    error_correction = forward_model(inputs)  # Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the silicobot on the (weighted) equal amounts and add the previously found error correction. Now we can calculate the difference between this corrected color and the original pumpbot color in the equal amounts case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix a color with silicobot and error correct using the model\n",
    "# Compare the color with the pumpbot\n",
    "corrected_equal_color = silicobot.mix_color(best_weights * torch.tensor([0.25, 0.25, 0.25, 0.25])) + error_correction.squeeze().tolist()\n",
    "score = color_difference(corrected_equal_color,pumpbot_equal_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize this color!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_rgb(mixture = equal_color_mixture,\n",
    "              rgb=corrected_equal_color,\n",
    "              pump_controller=silicobot,\n",
    "              target=pumpbot_equal_color,\n",
    "              score=score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew! That was a lot of work! Hopefully the difference between this error-corrected model and the original pumpbot color is very low!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create a Reverse NN model to predict color mixture from color measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully we now have a silicobot, that with error correction is as close of a digital twin as possible to the original pumpbot. We can now generate a bunch of random mixtures and then use our digital twin to predict the colors that the pumpbot might output. \n",
    "\n",
    "In the end, we want to be able to take a color measurement (RGB) of a target color, and figure out what the mixture to reproduce that color should be. After having generated the random mixtures and the resulting (error corrected) color from our digital twin, we can use the mixtures as an output and the color values as an input to train another NN model. The hope is that we can then take any color measurement, and this model will give us the mixture required to reproduce it! We will call this model the reverse model, to distinguish from the error correction model (forward model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5a. Generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by generating a lot of random mixtures, weighting them and normalizing. These values will be the outputs of our NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 1000\n",
    "Y_reverse = best_weights * torch.rand(n_points,4)\n",
    "Y_reverse /= Y_reverse.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the color using the silicobot and the error correction from the previous NN model. These values will be the inputs of our NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reverse = []\n",
    "with torch.no_grad():\n",
    "    for i in range(len(Y_reverse)):\n",
    "        x = torch.tensor(silicobot.mix_color(Y_reverse[i])) + forward_model(Y_reverse[i].unsqueeze(0)).squeeze()\n",
    "        X_reverse.append(x)\n",
    "\n",
    "X_reverse = torch.stack(X_reverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, we create a train and test dataset, along with their trainloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train, and test data for reverse model\n",
    "reverse_dataset = TensorDataset(X_reverse, Y_reverse)\n",
    "\n",
    "reverse_total_size = len(reverse_dataset)\n",
    "reverse_train_size = int(0.8 * reverse_total_size)\n",
    "reverse_test_size = reverse_total_size - reverse_train_size\n",
    "\n",
    "reverse_train_dataset, reverse_test_dataset = random_split(reverse_dataset, [reverse_train_size, reverse_test_size])\n",
    "\n",
    "# Dataloaders\n",
    "reverse_train_loader = DataLoader(reverse_train_dataset, batch_size=reverse_train_size, shuffle=True) # Use all data\n",
    "reverse_test_loader = DataLoader(reverse_test_dataset, batch_size=reverse_test_size, shuffle=False) # Use all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5b. Train the reverse model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the objective function to optimize the hidden layers as with the forward model. This time, the difference is that the MLPRegressor input layer has three nodes, one for each color in the measurement, and the output layer has four nodes, one for each mixture color. The model with the given hidden layer configuration is trained for 100 epochs the model is evaluated on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_reverse_model(hidden_layers):\n",
    "    # Round the number of nodes in each hidden layer to the nearest integer\n",
    "    hidden_layers = hidden_layers.to(torch.int64).squeeze()\n",
    "\n",
    "    # Create an instance of the MLP regressor\n",
    "    model = MLPRegressor(3, 4, hidden_layers)\n",
    "\n",
    "    model.train()\n",
    "    # Train the model\n",
    "    loss_fn = nn.MSELoss() # Mean squared error loss\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    n_epochs = 100\n",
    "    for epoch in range(n_epochs):\n",
    "        for inputs, targets in reverse_train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss = torch.sqrt(loss) # turn mse to rmse loss for each batch\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    # Evaluate the model\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in reverse_test_loader:\n",
    "            outputs = model(inputs)\n",
    "            total_loss += torch.sqrt(loss_fn(outputs, targets)) # turn mse to rmse loss for each batch\n",
    "\n",
    "    return total_loss / len(reverse_test_loader) # average rmse over all batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_objective = Objective(train_and_evaluate_reverse_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the hidden layer configuration is optimized using odyssey, with 5 maximum hidden layers, with each layer having 0 to 20 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goals = ['descend']\n",
    "\n",
    "min_hidden_layer_nodes = 0.0\n",
    "max_hidden_layer_nodes = 20.0\n",
    "max_n_hidden_layers = 5\n",
    "\n",
    "param_space = [[min_hidden_layer_nodes, max_hidden_layer_nodes]] * max_n_hidden_layers\n",
    "\n",
    "# Define Mission\n",
    "mission = Mission(name = 'RGB2Mix MLP Tuning',\n",
    "                  funcs = [reverse_objective],\n",
    "                  maneuvers = goals,\n",
    "                  envelope = param_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_init_design = 5\n",
    "navigator = SingleGP_Navigator(mission = mission,\n",
    "                               num_init_design = num_init_design,\n",
    "                               init_method = Sobol_Navigator(mission = mission),\n",
    "                               input_scaling = False,\n",
    "                               data_standardization = False,\n",
    "                               display_always_max = False,\n",
    "                               acq_function_type = ExpectedImprovement,\n",
    "                               acq_function_params = {'best_f': 0.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 15\n",
    "while len(mission.train_X) - num_init_design < num_iter:\n",
    "\n",
    "    with catch_warnings() as w:\n",
    "        simplefilter('ignore')\n",
    "        \n",
    "        trajectory = navigator.trajectory()\n",
    "        observation = navigator.probe(trajectory, init = False)\n",
    "\n",
    "        navigator.relay(trajectory, observation)\n",
    "        navigator.upgrade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best hidden layers configuration can now be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = mission.display_Y.argmin()\n",
    "best_params = mission.display_X[best_idx]\n",
    "best_metric = mission.display_Y[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params.to(torch.int64))\n",
    "print(best_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model is created with the best achieved hidden layer configuration and trained for more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the best model\n",
    "reverse_model = MLPRegressor(3, 4, best_params.to(torch.int64))\n",
    "\n",
    "reverse_model.train()\n",
    "# Train the model\n",
    "loss_fn = nn.MSELoss() # Mean squared error loss\n",
    "\n",
    "optimizer = optim.Adam(reverse_model.parameters())\n",
    "n_epochs = 500\n",
    "for epoch in range(n_epochs):\n",
    "    for inputs, targets in reverse_train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = reverse_model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss = torch.sqrt(loss) # turn mse to rmse loss for each batch\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Apply the reverse model on the pumpbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by adding a target color to the test_cell and measure the color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measured_color = pumpbot.measure()\n",
    "print(measured_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the proposed color mixture of this color using the reverse model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_model.eval()\n",
    "with torch.no_grad():\n",
    "    color_to_mix = reverse_model(measured_color)\n",
    "    print(color_to_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the pumpbot to mix the proposed mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_color = pumpbot.mix_color(color_to_mix)\n",
    "print(mixed_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the newly mixed color to the target color measurement. Hopefully the colors are identical both visually and in terms of the score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_rgb(mixture = color_to_mix.squeeze().tolist(),\n",
    "              rgb = mixed_color,\n",
    "              pump_controller=pumpbot,\n",
    "              target=measured_color.squeeze().tolist(),\n",
    "              score=color_difference(mixed_color, measured_color))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of work went to getting to this point, and even if you did not get a color that matched the original color, don't worry! You have learned a lot about digital twins, neural network models and using bayesian optimization for tuning hyperparameters and so much more!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
